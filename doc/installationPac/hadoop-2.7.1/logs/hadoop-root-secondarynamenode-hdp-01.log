2019-10-22 22:33:55,956 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = hdp-01/192.168.245.130
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop-2.7.1/etc/hadoop:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_211
************************************************************/
2019-10-22 22:33:55,965 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-22 22:33:57,227 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-22 22:33:57,453 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-10-22 22:33:57,454 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2019-10-22 22:33:58,241 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/namesecondary/in_use.lock acquired by nodename 30643@hdp-01
2019-10-22 22:33:58,245 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-10-22 22:33:58,245 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-10-22 22:33:58,306 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-10-22 22:33:58,307 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-10-22 22:33:58,308 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-10-22 22:33:58,311 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Oct 22 22:33:58
2019-10-22 22:33:58,314 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-10-22 22:33:58,314 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-10-22 22:33:58,334 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2019-10-22 22:33:58,334 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-10-22 22:33:58,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-10-22 22:33:58,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2019-10-22 22:33:58,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-10-22 22:33:58,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-10-22 22:33:58,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-10-22 22:33:58,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2019-10-22 22:33:58,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-10-22 22:33:58,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-10-22 22:33:58,920 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-10-22 22:33:58,922 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-10-22 22:33:58,923 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-10-22 22:33:58,923 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-10-22 22:33:58,923 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-10-22 22:33:58,927 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-10-22 22:33:59,546 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-10-22 22:33:59,546 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-10-22 22:33:59,546 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2019-10-22 22:33:59,546 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-10-22 22:33:59,547 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-10-22 22:33:59,547 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-10-22 22:33:59,547 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-10-22 22:33:59,547 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-10-22 22:33:59,578 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-10-22 22:33:59,578 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-10-22 22:33:59,578 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2019-10-22 22:33:59,578 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2019-10-22 22:33:59,580 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-10-22 22:33:59,580 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-10-22 22:33:59,580 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-10-22 22:33:59,583 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-10-22 22:33:59,583 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-10-22 22:33:59,583 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-10-22 22:33:59,729 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2019-10-22 22:33:59,931 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-22 22:34:00,117 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-22 22:34:00,137 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2019-10-22 22:34:00,185 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-22 22:34:00,188 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2019-10-22 22:34:00,188 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-22 22:34:00,188 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-22 22:34:00,247 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2019-10-22 22:34:00,247 INFO org.mortbay.log: jetty-6.1.26
2019-10-22 22:34:00,936 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2019-10-22 22:34:00,936 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2019-10-22 22:34:01,076 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2019-10-22 22:34:01,076 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2019-10-22 22:35:02,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:35:03,137 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:35:04,138 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:35:05,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:35:06,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:35:07,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:35:08,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:35:09,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:35:10,145 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:35:11,146 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:35:11,149 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From hdp-01/192.168.245.130 to hdp-01:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 18 more
2019-10-22 22:36:12,225 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:36:13,226 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:36:14,227 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:36:15,228 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:36:16,229 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:36:17,230 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:36:18,231 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:36:19,232 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:36:20,233 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:36:21,234 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:36:21,235 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From hdp-01/192.168.245.130 to hdp-01:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 18 more
2019-10-22 22:37:22,238 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:37:23,239 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:37:24,240 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:37:25,241 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:37:26,242 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:37:27,243 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:37:28,244 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:37:29,245 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:37:30,246 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:37:31,247 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:37:31,248 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From hdp-01/192.168.245.130 to hdp-01:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 18 more
2019-10-22 22:38:32,251 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:38:33,253 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:38:34,253 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:38:35,254 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:38:36,256 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:38:37,258 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:38:38,258 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:38:39,260 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:38:40,260 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:38:41,262 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:38:41,263 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From hdp-01/192.168.245.130 to hdp-01:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 18 more
2019-10-22 22:39:42,266 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:39:43,268 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:39:44,270 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:39:45,272 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:39:46,273 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:39:47,274 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:39:48,275 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:39:49,276 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:39:50,277 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:39:51,278 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:39:51,279 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From hdp-01/192.168.245.130 to hdp-01:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 18 more
2019-10-22 22:40:52,283 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:40:53,284 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:40:54,285 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:40:55,286 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:40:56,287 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:40:57,289 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:40:58,290 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:40:59,291 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:41:00,292 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:41:01,293 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:41:01,294 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From hdp-01/192.168.245.130 to hdp-01:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 18 more
2019-10-22 22:42:02,298 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:42:03,299 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:42:04,300 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:42:05,301 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:42:06,302 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:42:07,303 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:42:08,304 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:42:09,305 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:42:10,306 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:42:11,307 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:42:11,309 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From hdp-01/192.168.245.130 to hdp-01:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 18 more
2019-10-22 22:43:12,312 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:43:13,313 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:43:14,314 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:43:15,315 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:43:16,316 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:43:17,317 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:43:18,319 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:43:19,320 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:43:20,321 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:43:21,322 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:43:21,323 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From hdp-01/192.168.245.130 to hdp-01:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 18 more
2019-10-22 22:44:22,327 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:44:23,328 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:44:24,329 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:44:25,332 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:44:26,334 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:44:27,335 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:44:28,336 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:44:29,337 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:44:30,338 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:44:31,339 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:44:31,340 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From hdp-01/192.168.245.130 to hdp-01:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 18 more
2019-10-22 22:45:32,343 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:45:33,344 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:45:34,345 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:45:35,346 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:45:36,347 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:45:37,348 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:45:38,349 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:45:39,350 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:45:40,351 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:45:41,353 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:45:41,354 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From hdp-01/192.168.245.130 to hdp-01:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 18 more
2019-10-22 22:46:42,359 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:46:43,360 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:46:44,361 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:46:45,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:46:46,363 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:46:47,364 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:46:48,365 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:46:49,366 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:46:50,367 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:46:51,368 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:46:51,369 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From hdp-01/192.168.245.130 to hdp-01:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 18 more
2019-10-22 22:47:52,372 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:47:53,374 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:47:54,374 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:47:55,375 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:47:56,376 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:47:57,378 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:47:58,379 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:47:59,381 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:48:00,382 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:48:01,383 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:48:01,384 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From hdp-01/192.168.245.130 to hdp-01:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 18 more
2019-10-22 22:49:02,386 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:49:03,387 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:49:04,390 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:49:05,391 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:49:06,392 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:49:07,393 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:49:08,394 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:49:09,395 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:49:10,396 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:49:11,397 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:49:11,398 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From hdp-01/192.168.245.130 to hdp-01:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 18 more
2019-10-22 22:50:12,402 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:50:13,403 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:50:14,404 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:50:15,406 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:50:16,407 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:50:17,408 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:50:18,409 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:50:19,410 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:50:20,411 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:50:21,412 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:50:21,413 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From hdp-01/192.168.245.130 to hdp-01:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 18 more
2019-10-22 22:51:22,417 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:51:23,418 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:51:24,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:51:25,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:51:26,421 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:51:27,422 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:51:28,423 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:51:29,424 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:51:30,425 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:51:31,426 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:51:31,427 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From hdp-01/192.168.245.130 to hdp-01:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 18 more
2019-10-22 22:52:32,447 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:52:33,448 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:52:34,449 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:52:35,450 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:52:36,451 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:52:37,452 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:52:38,453 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:52:39,454 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:52:40,456 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:52:41,457 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:52:41,458 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From hdp-01/192.168.245.130 to hdp-01:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 18 more
2019-10-22 22:53:42,462 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:53:43,463 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:53:44,464 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:53:45,465 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:53:46,466 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:53:47,467 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:53:48,468 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:53:49,470 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:53:50,471 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:53:51,472 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:53:51,473 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From hdp-01/192.168.245.130 to hdp-01:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor13.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 17 more
2019-10-22 22:54:52,476 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:54:53,477 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:54:54,478 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:54:55,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:54:56,481 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:54:57,482 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:54:58,483 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:54:59,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:55:00,485 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:55:01,486 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:55:01,487 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From hdp-01/192.168.245.130 to hdp-01:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor13.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 17 more
2019-10-22 22:56:02,491 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:56:03,508 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:56:04,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:56:05,528 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:56:06,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:56:07,532 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:56:08,533 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:56:09,534 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:56:10,537 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:56:11,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: hdp-01/192.168.245.130:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-10-22 22:56:11,540 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From hdp-01/192.168.245.130 to hdp-01:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor13.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1407)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:128)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:641)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:649)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:393)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:361)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:357)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:609)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:707)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:370)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1446)
	... 17 more
2019-10-22 22:56:32,970 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2019-10-22 22:56:32,973 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at hdp-01/192.168.245.130
************************************************************/
2019-10-22 23:07:49,557 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   host = hdp-01/192.168.245.130
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.1
STARTUP_MSG:   classpath = /usr/local/hadoop-2.7.1/etc/hadoop:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-auth-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/common/hadoop-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/hdfs/hadoop-hdfs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-api-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-client-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-registry-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.1-tests.jar:/usr/local/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.1.jar:/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar:/usr/local/hadoop-2.7.1/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 15ecc87ccf4a0228f35af08fc56de536e6ce657a; compiled by 'jenkins' on 2015-06-29T06:04Z
STARTUP_MSG:   java = 1.8.0_211
************************************************************/
2019-10-22 23:07:49,565 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-22 23:07:50,600 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-10-22 23:07:50,759 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2019-10-22 23:07:50,759 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2019-10-22 23:07:50,982 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-root/dfs/namesecondary/in_use.lock acquired by nodename 32739@hdp-01
2019-10-22 23:07:50,986 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2019-10-22 23:07:50,986 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2019-10-22 23:07:51,046 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2019-10-22 23:07:51,047 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-10-22 23:07:51,048 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-10-22 23:07:51,049 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2019 Oct 22 23:07:51
2019-10-22 23:07:51,052 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2019-10-22 23:07:51,052 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-10-22 23:07:51,058 INFO org.apache.hadoop.util.GSet: 2.0% max memory 966.7 MB = 19.3 MB
2019-10-22 23:07:51,058 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2019-10-22 23:07:51,080 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2019-10-22 23:07:51,080 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2019-10-22 23:07:51,080 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2019-10-22 23:07:51,080 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2019-10-22 23:07:51,080 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-10-22 23:07:51,080 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: shouldCheckForEnoughRacks  = false
2019-10-22 23:07:51,080 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2019-10-22 23:07:51,080 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2019-10-22 23:07:51,080 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-10-22 23:07:51,082 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2019-10-22 23:07:51,082 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2019-10-22 23:07:51,082 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2019-10-22 23:07:51,082 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2019-10-22 23:07:51,091 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2019-10-22 23:07:51,525 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2019-10-22 23:07:51,525 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-10-22 23:07:51,525 INFO org.apache.hadoop.util.GSet: 1.0% max memory 966.7 MB = 9.7 MB
2019-10-22 23:07:51,525 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2019-10-22 23:07:51,526 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2019-10-22 23:07:51,526 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2019-10-22 23:07:51,527 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2019-10-22 23:07:51,528 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2019-10-22 23:07:51,535 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2019-10-22 23:07:51,535 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2019-10-22 23:07:51,536 INFO org.apache.hadoop.util.GSet: 0.25% max memory 966.7 MB = 2.4 MB
2019-10-22 23:07:51,536 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2019-10-22 23:07:51,538 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-10-22 23:07:51,538 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2019-10-22 23:07:51,538 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2019-10-22 23:07:51,560 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-10-22 23:07:51,560 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-10-22 23:07:51,560 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-10-22 23:07:51,580 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2019-10-22 23:07:51,800 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-10-22 23:07:51,823 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-10-22 23:07:51,839 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2019-10-22 23:07:51,844 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-10-22 23:07:51,851 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2019-10-22 23:07:51,851 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-10-22 23:07:51,851 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-10-22 23:07:51,878 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2019-10-22 23:07:51,878 INFO org.mortbay.log: jetty-6.1.26
2019-10-22 23:07:52,095 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2019-10-22 23:07:52,095 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2019-10-22 23:07:52,210 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2019-10-22 23:07:52,210 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2019-10-22 23:08:52,418 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2019-10-22 23:08:52,789 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getimage=1&txid=5&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-22 23:08:52,946 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2019-10-22 23:08:53,516 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.07s at 0.00 KB/s
2019-10-22 23:08:53,517 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000005 size 482 bytes.
2019-10-22 23:08:53,559 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=6&endTxId=7&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-22 23:08:53,565 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-22 23:08:53,565 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000006-0000000000000000007_0000000000434678834 size 0 bytes.
2019-10-22 23:08:53,617 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 3 INodes.
2019-10-22 23:08:53,651 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2019-10-22 23:08:53,651 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 5 from /tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000005
2019-10-22 23:08:53,651 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2019-10-22 23:08:53,664 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-22 23:08:53,668 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000006-0000000000000000007 expecting start txid #6
2019-10-22 23:08:53,668 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000006-0000000000000000007
2019-10-22 23:08:53,691 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000006-0000000000000000007 of size 42 edits # 2 loaded in 0 seconds
2019-10-22 23:08:53,755 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-root/dfs/namesecondary
2019-10-22 23:08:53,788 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 7 to namenode at http://hdp-01:50070 in 0.024 seconds
2019-10-22 23:08:53,788 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 482
2019-10-23 00:08:54,326 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 00:08:54,327 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=8&endTxId=61&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 00:08:54,341 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 2000.00 KB/s
2019-10-23 00:08:54,341 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000008-0000000000000000061_0000000000438279603 size 0 bytes.
2019-10-23 00:08:54,342 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 00:08:54,342 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000008-0000000000000000061 expecting start txid #8
2019-10-23 00:08:54,342 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000008-0000000000000000061
2019-10-23 00:08:54,481 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000008-0000000000000000061 of size 6678 edits # 54 loaded in 0 seconds
2019-10-23 00:08:54,530 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 7
2019-10-23 00:08:54,530 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000005, cpktTxId=0000000000000000005)
2019-10-23 00:08:54,593 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 61 to namenode at http://hdp-01:50070 in 0.059 seconds
2019-10-23 00:08:54,594 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 01:08:55,006 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 01:08:55,007 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=62&endTxId=63&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 01:08:55,015 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-23 01:08:55,015 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000062-0000000000000000063_0000000000441880282 size 0 bytes.
2019-10-23 01:08:55,016 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 01:08:55,016 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000062-0000000000000000063 expecting start txid #62
2019-10-23 01:08:55,016 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000062-0000000000000000063
2019-10-23 01:08:55,017 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000062-0000000000000000063 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 01:08:55,029 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 61
2019-10-23 01:08:55,029 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000007, cpktTxId=0000000000000000007)
2019-10-23 01:08:55,091 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 63 to namenode at http://hdp-01:50070 in 0.055 seconds
2019-10-23 01:08:55,092 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 02:08:55,477 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 02:08:55,478 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=64&endTxId=65&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 02:08:55,483 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-23 02:08:55,483 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000064-0000000000000000065_0000000000445480753 size 0 bytes.
2019-10-23 02:08:55,483 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 02:08:55,483 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000064-0000000000000000065 expecting start txid #64
2019-10-23 02:08:55,484 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000064-0000000000000000065
2019-10-23 02:08:55,484 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000064-0000000000000000065 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 02:08:55,500 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 63
2019-10-23 02:08:55,500 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000061, cpktTxId=0000000000000000061)
2019-10-23 02:08:55,513 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 65 to namenode at http://hdp-01:50070 in 0.01 seconds
2019-10-23 02:08:55,513 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 03:08:55,819 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 03:08:55,819 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=66&endTxId=67&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 03:08:55,872 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2019-10-23 03:08:55,872 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000066-0000000000000000067_0000000000449081095 size 0 bytes.
2019-10-23 03:08:55,872 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 03:08:55,872 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000066-0000000000000000067 expecting start txid #66
2019-10-23 03:08:55,872 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000066-0000000000000000067
2019-10-23 03:08:55,873 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000066-0000000000000000067 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 03:08:55,884 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 65
2019-10-23 03:08:55,884 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000063, cpktTxId=0000000000000000063)
2019-10-23 03:08:55,906 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 67 to namenode at http://hdp-01:50070 in 0.015 seconds
2019-10-23 03:08:55,906 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 04:08:56,186 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 04:08:56,187 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=68&endTxId=69&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 04:08:56,193 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-23 04:08:56,193 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000068-0000000000000000069_0000000000452681462 size 0 bytes.
2019-10-23 04:08:56,193 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 04:08:56,193 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000068-0000000000000000069 expecting start txid #68
2019-10-23 04:08:56,194 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000068-0000000000000000069
2019-10-23 04:08:56,194 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000068-0000000000000000069 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 04:08:56,204 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 67
2019-10-23 04:08:56,204 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000065, cpktTxId=0000000000000000065)
2019-10-23 04:08:56,272 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 69 to namenode at http://hdp-01:50070 in 0.066 seconds
2019-10-23 04:08:56,273 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 05:08:56,504 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 05:08:56,504 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=70&endTxId=71&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 05:08:56,556 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2019-10-23 05:08:56,556 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000070-0000000000000000071_0000000000456281780 size 0 bytes.
2019-10-23 05:08:56,557 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 05:08:56,557 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000070-0000000000000000071 expecting start txid #70
2019-10-23 05:08:56,557 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000070-0000000000000000071
2019-10-23 05:08:56,557 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000070-0000000000000000071 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 05:08:56,564 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 69
2019-10-23 05:08:56,564 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000067, cpktTxId=0000000000000000067)
2019-10-23 05:08:56,580 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 71 to namenode at http://hdp-01:50070 in 0.008 seconds
2019-10-23 05:08:56,580 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 06:08:56,853 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 06:08:56,854 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=72&endTxId=73&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 06:08:56,859 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-23 06:08:56,859 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000072-0000000000000000073_0000000000459882129 size 0 bytes.
2019-10-23 06:08:56,859 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 06:08:56,859 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000072-0000000000000000073 expecting start txid #72
2019-10-23 06:08:56,859 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000072-0000000000000000073
2019-10-23 06:08:56,860 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000072-0000000000000000073 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 06:08:56,872 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 71
2019-10-23 06:08:56,872 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000069, cpktTxId=0000000000000000069)
2019-10-23 06:08:56,930 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 73 to namenode at http://hdp-01:50070 in 0.055 seconds
2019-10-23 06:08:56,930 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 07:08:57,146 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 07:08:57,146 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=74&endTxId=75&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 07:08:57,152 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-23 07:08:57,152 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000074-0000000000000000075_0000000000463482422 size 0 bytes.
2019-10-23 07:08:57,152 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 07:08:57,152 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000074-0000000000000000075 expecting start txid #74
2019-10-23 07:08:57,152 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000074-0000000000000000075
2019-10-23 07:08:57,153 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000074-0000000000000000075 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 07:08:57,161 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 73
2019-10-23 07:08:57,162 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000071, cpktTxId=0000000000000000071)
2019-10-23 07:08:57,218 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 75 to namenode at http://hdp-01:50070 in 0.055 seconds
2019-10-23 07:08:57,218 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 08:08:57,474 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 08:08:57,475 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=76&endTxId=77&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 08:08:57,522 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2019-10-23 08:08:57,522 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000076-0000000000000000077_0000000000467082750 size 0 bytes.
2019-10-23 08:08:57,522 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 08:08:57,522 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000076-0000000000000000077 expecting start txid #76
2019-10-23 08:08:57,523 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000076-0000000000000000077
2019-10-23 08:08:57,523 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000076-0000000000000000077 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 08:08:57,530 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 75
2019-10-23 08:08:57,530 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000073, cpktTxId=0000000000000000073)
2019-10-23 08:08:57,542 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 77 to namenode at http://hdp-01:50070 in 0.01 seconds
2019-10-23 08:08:57,542 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 09:08:57,743 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 09:08:57,744 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=78&endTxId=79&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 09:08:57,792 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 0.00 KB/s
2019-10-23 09:08:57,792 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000078-0000000000000000079_0000000000470683019 size 0 bytes.
2019-10-23 09:08:57,792 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 09:08:57,792 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000078-0000000000000000079 expecting start txid #78
2019-10-23 09:08:57,792 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000078-0000000000000000079
2019-10-23 09:08:57,793 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000078-0000000000000000079 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 09:08:57,803 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 77
2019-10-23 09:08:57,803 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000075, cpktTxId=0000000000000000075)
2019-10-23 09:08:57,825 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 79 to namenode at http://hdp-01:50070 in 0.016 seconds
2019-10-23 09:08:57,825 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 10:08:58,121 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 10:08:58,121 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=80&endTxId=81&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 10:08:58,139 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 0.00 KB/s
2019-10-23 10:08:58,139 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000080-0000000000000000081_0000000000474283397 size 0 bytes.
2019-10-23 10:08:58,139 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 10:08:58,139 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000080-0000000000000000081 expecting start txid #80
2019-10-23 10:08:58,139 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000080-0000000000000000081
2019-10-23 10:08:58,140 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000080-0000000000000000081 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 10:08:58,149 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 79
2019-10-23 10:08:58,149 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000077, cpktTxId=0000000000000000077)
2019-10-23 10:08:58,166 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 81 to namenode at http://hdp-01:50070 in 0.015 seconds
2019-10-23 10:08:58,167 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 11:08:58,349 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 11:08:58,350 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=82&endTxId=83&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 11:08:58,362 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-23 11:08:58,362 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000082-0000000000000000083_0000000000477883626 size 0 bytes.
2019-10-23 11:08:58,363 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 11:08:58,363 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000082-0000000000000000083 expecting start txid #82
2019-10-23 11:08:58,363 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000082-0000000000000000083
2019-10-23 11:08:58,363 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000082-0000000000000000083 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 11:08:58,373 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 81
2019-10-23 11:08:58,373 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000079, cpktTxId=0000000000000000079)
2019-10-23 11:08:58,389 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 83 to namenode at http://hdp-01:50070 in 0.012 seconds
2019-10-23 11:08:58,389 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 12:08:58,613 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 12:08:58,614 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=84&endTxId=85&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 12:08:58,620 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-23 12:08:58,620 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000084-0000000000000000085_0000000000481483889 size 0 bytes.
2019-10-23 12:08:58,620 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 12:08:58,620 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000084-0000000000000000085 expecting start txid #84
2019-10-23 12:08:58,620 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000084-0000000000000000085
2019-10-23 12:08:58,620 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000084-0000000000000000085 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 12:08:58,630 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 83
2019-10-23 12:08:58,630 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000081, cpktTxId=0000000000000000081)
2019-10-23 12:08:58,651 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 85 to namenode at http://hdp-01:50070 in 0.019 seconds
2019-10-23 12:08:58,651 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 13:08:58,918 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 13:08:58,918 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=86&endTxId=87&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 13:08:58,924 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-23 13:08:58,924 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000086-0000000000000000087_0000000000485084194 size 0 bytes.
2019-10-23 13:08:58,924 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 13:08:58,924 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000086-0000000000000000087 expecting start txid #86
2019-10-23 13:08:58,924 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000086-0000000000000000087
2019-10-23 13:08:58,925 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000086-0000000000000000087 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 13:08:58,934 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 85
2019-10-23 13:08:58,934 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000083, cpktTxId=0000000000000000083)
2019-10-23 13:08:58,950 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 87 to namenode at http://hdp-01:50070 in 0.01 seconds
2019-10-23 13:08:58,950 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 14:08:59,140 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 14:08:59,140 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=88&endTxId=89&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 14:08:59,147 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-23 14:08:59,147 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000088-0000000000000000089_0000000000488684416 size 0 bytes.
2019-10-23 14:08:59,147 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 14:08:59,147 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000088-0000000000000000089 expecting start txid #88
2019-10-23 14:08:59,147 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000088-0000000000000000089
2019-10-23 14:08:59,148 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000088-0000000000000000089 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 14:08:59,164 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 87
2019-10-23 14:08:59,164 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000085, cpktTxId=0000000000000000085)
2019-10-23 14:08:59,176 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 89 to namenode at http://hdp-01:50070 in 0.007 seconds
2019-10-23 14:08:59,177 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 15:08:59,414 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 15:08:59,415 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=90&endTxId=91&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 15:08:59,421 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-23 15:08:59,422 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000090-0000000000000000091_0000000000492284690 size 0 bytes.
2019-10-23 15:08:59,422 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 15:08:59,422 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000090-0000000000000000091 expecting start txid #90
2019-10-23 15:08:59,422 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000090-0000000000000000091
2019-10-23 15:08:59,422 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000090-0000000000000000091 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 15:08:59,431 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 89
2019-10-23 15:08:59,431 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000087, cpktTxId=0000000000000000087)
2019-10-23 15:08:59,471 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 91 to namenode at http://hdp-01:50070 in 0.025 seconds
2019-10-23 15:08:59,471 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 16:08:59,645 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 16:08:59,645 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=92&endTxId=93&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 16:08:59,651 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-23 16:08:59,651 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000092-0000000000000000093_0000000000495884921 size 0 bytes.
2019-10-23 16:08:59,651 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 16:08:59,651 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000092-0000000000000000093 expecting start txid #92
2019-10-23 16:08:59,651 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000092-0000000000000000093
2019-10-23 16:08:59,651 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000092-0000000000000000093 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 16:08:59,663 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 91
2019-10-23 16:08:59,663 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000089, cpktTxId=0000000000000000089)
2019-10-23 16:08:59,680 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 93 to namenode at http://hdp-01:50070 in 0.013 seconds
2019-10-23 16:08:59,680 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 17:08:59,920 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 17:08:59,920 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=94&endTxId=95&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 17:08:59,945 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-23 17:08:59,945 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000094-0000000000000000095_0000000000499485196 size 0 bytes.
2019-10-23 17:08:59,945 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 17:08:59,945 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000094-0000000000000000095 expecting start txid #94
2019-10-23 17:08:59,945 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000094-0000000000000000095
2019-10-23 17:08:59,946 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000094-0000000000000000095 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 17:08:59,960 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 93
2019-10-23 17:08:59,961 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000091, cpktTxId=0000000000000000091)
2019-10-23 17:08:59,976 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 95 to namenode at http://hdp-01:50070 in 0.011 seconds
2019-10-23 17:08:59,976 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 18:09:00,166 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 18:09:00,166 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=96&endTxId=97&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 18:09:00,173 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-23 18:09:00,173 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000096-0000000000000000097_0000000000503085442 size 0 bytes.
2019-10-23 18:09:00,173 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 18:09:00,173 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000096-0000000000000000097 expecting start txid #96
2019-10-23 18:09:00,173 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000096-0000000000000000097
2019-10-23 18:09:00,174 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000096-0000000000000000097 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 18:09:00,181 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 95
2019-10-23 18:09:00,182 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000093, cpktTxId=0000000000000000093)
2019-10-23 18:09:00,200 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 97 to namenode at http://hdp-01:50070 in 0.012 seconds
2019-10-23 18:09:00,200 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 19:09:00,372 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 19:09:00,373 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=98&endTxId=99&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 19:09:00,397 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.02s at 0.00 KB/s
2019-10-23 19:09:00,397 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000098-0000000000000000099_0000000000506685648 size 0 bytes.
2019-10-23 19:09:00,397 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 19:09:00,397 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000098-0000000000000000099 expecting start txid #98
2019-10-23 19:09:00,397 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000098-0000000000000000099
2019-10-23 19:09:00,398 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000098-0000000000000000099 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 19:09:00,403 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 97
2019-10-23 19:09:00,403 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000095, cpktTxId=0000000000000000095)
2019-10-23 19:09:00,422 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 99 to namenode at http://hdp-01:50070 in 0.01 seconds
2019-10-23 19:09:00,422 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 20:09:00,606 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 20:09:00,606 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=100&endTxId=101&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 20:09:00,616 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 0.00 KB/s
2019-10-23 20:09:00,617 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000100-0000000000000000101_0000000000510285882 size 0 bytes.
2019-10-23 20:09:00,617 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 20:09:00,617 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000100-0000000000000000101 expecting start txid #100
2019-10-23 20:09:00,617 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000100-0000000000000000101
2019-10-23 20:09:00,617 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000100-0000000000000000101 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 20:09:00,634 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 99
2019-10-23 20:09:00,634 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000097, cpktTxId=0000000000000000097)
2019-10-23 20:09:00,667 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 101 to namenode at http://hdp-01:50070 in 0.025 seconds
2019-10-23 20:09:00,667 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 21:09:00,908 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 21:09:00,908 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=102&endTxId=103&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 21:09:00,963 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.02s at 0.00 KB/s
2019-10-23 21:09:00,964 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000102-0000000000000000103_0000000000513886184 size 0 bytes.
2019-10-23 21:09:00,965 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 21:09:00,965 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000102-0000000000000000103 expecting start txid #102
2019-10-23 21:09:00,965 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000102-0000000000000000103
2019-10-23 21:09:00,965 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000102-0000000000000000103 of size 42 edits # 2 loaded in 0 seconds
2019-10-23 21:09:00,985 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 101
2019-10-23 21:09:00,985 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000099, cpktTxId=0000000000000000099)
2019-10-23 21:09:01,019 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 103 to namenode at http://hdp-01:50070 in 0.027 seconds
2019-10-23 21:09:01,019 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1060
2019-10-23 22:09:01,537 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 22:09:01,538 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=104&endTxId=107&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 22:09:01,543 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-23 22:09:01,543 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000104-0000000000000000107_0000000000517486813 size 0 bytes.
2019-10-23 22:09:01,543 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 22:09:01,544 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000104-0000000000000000107 expecting start txid #104
2019-10-23 22:09:01,544 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000104-0000000000000000107
2019-10-23 22:09:01,545 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000104-0000000000000000107 of size 290 edits # 4 loaded in 0 seconds
2019-10-23 22:09:01,557 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 103
2019-10-23 22:09:01,557 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000101, cpktTxId=0000000000000000101)
2019-10-23 22:09:01,579 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 107 to namenode at http://hdp-01:50070 in 0.014 seconds
2019-10-23 22:09:01,579 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1227
2019-10-23 23:09:01,876 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-23 23:09:01,877 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=108&endTxId=121&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-23 23:09:01,884 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 1000.00 KB/s
2019-10-23 23:09:01,884 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000108-0000000000000000121_0000000000521087153 size 0 bytes.
2019-10-23 23:09:01,884 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-23 23:09:01,884 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000108-0000000000000000121 expecting start txid #108
2019-10-23 23:09:01,885 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000108-0000000000000000121
2019-10-23 23:09:01,888 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000108-0000000000000000121 of size 1167 edits # 14 loaded in 0 seconds
2019-10-23 23:09:01,904 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 107
2019-10-23 23:09:01,904 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000103, cpktTxId=0000000000000000103)
2019-10-23 23:09:01,923 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 121 to namenode at http://hdp-01:50070 in 0.015 seconds
2019-10-23 23:09:01,924 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1415
2019-10-24 00:09:02,396 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 00:09:02,397 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=122&endTxId=123&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 00:09:02,448 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-24 00:09:02,448 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000122-0000000000000000123_0000000000524687673 size 0 bytes.
2019-10-24 00:09:02,448 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 00:09:02,448 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000122-0000000000000000123 expecting start txid #122
2019-10-24 00:09:02,448 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000122-0000000000000000123
2019-10-24 00:09:02,449 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000122-0000000000000000123 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 00:09:02,481 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 121
2019-10-24 00:09:02,481 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000107, cpktTxId=0000000000000000107)
2019-10-24 00:09:02,549 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 123 to namenode at http://hdp-01:50070 in 0.037 seconds
2019-10-24 00:09:02,549 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1415
2019-10-24 01:09:02,857 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 01:09:02,858 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=124&endTxId=125&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 01:09:02,899 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 0.00 KB/s
2019-10-24 01:09:02,899 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000124-0000000000000000125_0000000000528288134 size 0 bytes.
2019-10-24 01:09:02,900 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 01:09:02,900 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000124-0000000000000000125 expecting start txid #124
2019-10-24 01:09:02,900 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000124-0000000000000000125
2019-10-24 01:09:02,900 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000124-0000000000000000125 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 01:09:02,933 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 123
2019-10-24 01:09:02,933 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000121, cpktTxId=0000000000000000121)
2019-10-24 01:09:02,958 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 125 to namenode at http://hdp-01:50070 in 0.008 seconds
2019-10-24 01:09:02,958 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1415
2019-10-24 02:09:03,321 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 02:09:03,321 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=126&endTxId=127&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 02:09:03,366 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-24 02:09:03,366 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000126-0000000000000000127_0000000000531888597 size 0 bytes.
2019-10-24 02:09:03,367 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 02:09:03,367 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000126-0000000000000000127 expecting start txid #126
2019-10-24 02:09:03,367 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000126-0000000000000000127
2019-10-24 02:09:03,367 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000126-0000000000000000127 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 02:09:03,387 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 125
2019-10-24 02:09:03,387 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000123, cpktTxId=0000000000000000123)
2019-10-24 02:09:03,404 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 127 to namenode at http://hdp-01:50070 in 0.013 seconds
2019-10-24 02:09:03,405 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1415
2019-10-24 03:09:03,725 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 03:09:03,725 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=128&endTxId=129&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 03:09:03,746 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-24 03:09:03,746 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000128-0000000000000000129_0000000000535489001 size 0 bytes.
2019-10-24 03:09:03,746 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 03:09:03,746 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000128-0000000000000000129 expecting start txid #128
2019-10-24 03:09:03,746 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000128-0000000000000000129
2019-10-24 03:09:03,746 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000128-0000000000000000129 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 03:09:03,755 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 127
2019-10-24 03:09:03,755 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000125, cpktTxId=0000000000000000125)
2019-10-24 03:09:03,788 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 129 to namenode at http://hdp-01:50070 in 0.02 seconds
2019-10-24 03:09:03,788 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-24 04:09:04,132 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 04:09:04,133 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=130&endTxId=131&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 04:09:04,149 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 0.00 KB/s
2019-10-24 04:09:04,149 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000130-0000000000000000131_0000000000539089408 size 0 bytes.
2019-10-24 04:09:04,149 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 04:09:04,149 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000130-0000000000000000131 expecting start txid #130
2019-10-24 04:09:04,149 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000130-0000000000000000131
2019-10-24 04:09:04,150 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000130-0000000000000000131 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 04:09:04,209 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 129
2019-10-24 04:09:04,209 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000127, cpktTxId=0000000000000000127)
2019-10-24 04:09:04,313 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 131 to namenode at http://hdp-01:50070 in 0.093 seconds
2019-10-24 04:09:04,313 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-24 05:09:04,724 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 05:09:04,725 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=132&endTxId=133&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 05:09:04,736 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 0.00 KB/s
2019-10-24 05:09:04,736 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000132-0000000000000000133_0000000000542690000 size 0 bytes.
2019-10-24 05:09:04,736 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 05:09:04,736 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000132-0000000000000000133 expecting start txid #132
2019-10-24 05:09:04,736 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000132-0000000000000000133
2019-10-24 05:09:04,737 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000132-0000000000000000133 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 05:09:04,749 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 131
2019-10-24 05:09:04,750 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000129, cpktTxId=0000000000000000129)
2019-10-24 05:09:04,816 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 133 to namenode at http://hdp-01:50070 in 0.04 seconds
2019-10-24 05:09:04,816 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-24 06:09:05,158 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 06:09:05,158 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=134&endTxId=135&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 06:09:05,208 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2019-10-24 06:09:05,208 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000134-0000000000000000135_0000000000546290434 size 0 bytes.
2019-10-24 06:09:05,208 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 06:09:05,208 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000134-0000000000000000135 expecting start txid #134
2019-10-24 06:09:05,209 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000134-0000000000000000135
2019-10-24 06:09:05,209 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000134-0000000000000000135 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 06:09:05,219 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 133
2019-10-24 06:09:05,219 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000131, cpktTxId=0000000000000000131)
2019-10-24 06:09:05,252 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 135 to namenode at http://hdp-01:50070 in 0.02 seconds
2019-10-24 06:09:05,252 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-24 07:09:05,507 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 07:09:05,507 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=136&endTxId=137&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 07:09:05,558 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.05s at 0.00 KB/s
2019-10-24 07:09:05,558 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000136-0000000000000000137_0000000000549890783 size 0 bytes.
2019-10-24 07:09:05,558 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 07:09:05,558 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000136-0000000000000000137 expecting start txid #136
2019-10-24 07:09:05,558 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000136-0000000000000000137
2019-10-24 07:09:05,559 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000136-0000000000000000137 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 07:09:05,565 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 135
2019-10-24 07:09:05,565 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000133, cpktTxId=0000000000000000133)
2019-10-24 07:09:05,576 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 137 to namenode at http://hdp-01:50070 in 0.007 seconds
2019-10-24 07:09:05,576 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-24 08:09:05,801 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 08:09:05,802 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=138&endTxId=139&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 08:09:05,837 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 0.00 KB/s
2019-10-24 08:09:05,837 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000138-0000000000000000139_0000000000553491077 size 0 bytes.
2019-10-24 08:09:05,837 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 08:09:05,837 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000138-0000000000000000139 expecting start txid #138
2019-10-24 08:09:05,837 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000138-0000000000000000139
2019-10-24 08:09:05,838 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000138-0000000000000000139 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 08:09:05,858 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 137
2019-10-24 08:09:05,858 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000135, cpktTxId=0000000000000000135)
2019-10-24 08:09:05,880 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 139 to namenode at http://hdp-01:50070 in 0.019 seconds
2019-10-24 08:09:05,881 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-24 09:09:06,110 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 09:09:06,110 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=140&endTxId=141&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 09:09:06,141 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-24 09:09:06,141 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000140-0000000000000000141_0000000000557091386 size 0 bytes.
2019-10-24 09:09:06,141 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 09:09:06,141 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000140-0000000000000000141 expecting start txid #140
2019-10-24 09:09:06,141 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000140-0000000000000000141
2019-10-24 09:09:06,141 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000140-0000000000000000141 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 09:09:06,148 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 139
2019-10-24 09:09:06,148 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000137, cpktTxId=0000000000000000137)
2019-10-24 09:09:06,154 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 141 to namenode at http://hdp-01:50070 in 0.005 seconds
2019-10-24 09:09:06,154 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-24 10:09:06,416 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 10:09:06,416 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=142&endTxId=143&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 10:09:06,451 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.02s at 0.00 KB/s
2019-10-24 10:09:06,452 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000142-0000000000000000143_0000000000560691692 size 0 bytes.
2019-10-24 10:09:06,452 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 10:09:06,452 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000142-0000000000000000143 expecting start txid #142
2019-10-24 10:09:06,452 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000142-0000000000000000143
2019-10-24 10:09:06,452 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000142-0000000000000000143 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 10:09:06,485 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 141
2019-10-24 10:09:06,485 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000139, cpktTxId=0000000000000000139)
2019-10-24 10:09:06,516 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 143 to namenode at http://hdp-01:50070 in 0.03 seconds
2019-10-24 10:09:06,516 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-24 11:09:06,737 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 11:09:06,737 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=144&endTxId=145&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 11:09:06,742 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-24 11:09:06,742 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000144-0000000000000000145_0000000000564292013 size 0 bytes.
2019-10-24 11:09:06,742 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 11:09:06,742 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000144-0000000000000000145 expecting start txid #144
2019-10-24 11:09:06,742 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000144-0000000000000000145
2019-10-24 11:09:06,743 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000144-0000000000000000145 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 11:09:06,765 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 143
2019-10-24 11:09:06,765 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000141, cpktTxId=0000000000000000141)
2019-10-24 11:09:06,779 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 145 to namenode at http://hdp-01:50070 in 0.013 seconds
2019-10-24 11:09:06,780 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-24 12:09:06,998 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 12:09:06,999 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=146&endTxId=147&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 12:09:07,004 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-24 12:09:07,005 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000146-0000000000000000147_0000000000567892274 size 0 bytes.
2019-10-24 12:09:07,005 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 12:09:07,005 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000146-0000000000000000147 expecting start txid #146
2019-10-24 12:09:07,005 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000146-0000000000000000147
2019-10-24 12:09:07,005 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000146-0000000000000000147 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 12:09:07,319 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 145
2019-10-24 12:09:07,319 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000143, cpktTxId=0000000000000000143)
2019-10-24 12:09:07,338 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 147 to namenode at http://hdp-01:50070 in 0.01 seconds
2019-10-24 12:09:07,338 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-24 13:09:07,534 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 13:09:07,534 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=148&endTxId=149&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 13:09:07,538 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-24 13:09:07,538 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000148-0000000000000000149_0000000000571492810 size 0 bytes.
2019-10-24 13:09:07,538 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 13:09:07,538 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000148-0000000000000000149 expecting start txid #148
2019-10-24 13:09:07,538 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000148-0000000000000000149
2019-10-24 13:09:07,538 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000148-0000000000000000149 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 13:09:07,547 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 147
2019-10-24 13:09:07,547 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000145, cpktTxId=0000000000000000145)
2019-10-24 13:09:07,628 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 149 to namenode at http://hdp-01:50070 in 0.077 seconds
2019-10-24 13:09:07,628 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-24 14:09:07,894 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 14:09:07,894 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=150&endTxId=151&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 14:09:07,898 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-24 14:09:07,898 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000150-0000000000000000151_0000000000575093170 size 0 bytes.
2019-10-24 14:09:07,898 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 14:09:07,898 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000150-0000000000000000151 expecting start txid #150
2019-10-24 14:09:07,898 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000150-0000000000000000151
2019-10-24 14:09:07,899 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000150-0000000000000000151 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 14:09:07,908 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 149
2019-10-24 14:09:07,908 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000147, cpktTxId=0000000000000000147)
2019-10-24 14:09:07,950 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 151 to namenode at http://hdp-01:50070 in 0.028 seconds
2019-10-24 14:09:07,950 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-24 15:09:08,190 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 15:09:08,190 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=152&endTxId=153&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 15:09:08,195 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-24 15:09:08,195 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000152-0000000000000000153_0000000000578693466 size 0 bytes.
2019-10-24 15:09:08,195 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 15:09:08,195 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000152-0000000000000000153 expecting start txid #152
2019-10-24 15:09:08,195 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000152-0000000000000000153
2019-10-24 15:09:08,195 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000152-0000000000000000153 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 15:09:08,202 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 151
2019-10-24 15:09:08,203 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000149, cpktTxId=0000000000000000149)
2019-10-24 15:09:08,276 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 153 to namenode at http://hdp-01:50070 in 0.06 seconds
2019-10-24 15:09:08,276 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-24 16:09:08,513 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 16:09:08,513 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=154&endTxId=155&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 16:09:08,560 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.04s at 0.00 KB/s
2019-10-24 16:09:08,560 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000154-0000000000000000155_0000000000582293789 size 0 bytes.
2019-10-24 16:09:08,560 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 16:09:08,560 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000154-0000000000000000155 expecting start txid #154
2019-10-24 16:09:08,560 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000154-0000000000000000155
2019-10-24 16:09:08,561 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000154-0000000000000000155 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 16:09:08,568 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 153
2019-10-24 16:09:08,568 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000151, cpktTxId=0000000000000000151)
2019-10-24 16:09:08,589 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 155 to namenode at http://hdp-01:50070 in 0.019 seconds
2019-10-24 16:09:08,590 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-24 17:09:08,771 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 17:09:08,771 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=156&endTxId=157&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 17:09:08,780 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-24 17:09:08,780 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000156-0000000000000000157_0000000000585894047 size 0 bytes.
2019-10-24 17:09:08,780 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 17:09:08,780 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000156-0000000000000000157 expecting start txid #156
2019-10-24 17:09:08,780 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000156-0000000000000000157
2019-10-24 17:09:08,781 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000156-0000000000000000157 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 17:09:08,789 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 155
2019-10-24 17:09:08,789 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000153, cpktTxId=0000000000000000153)
2019-10-24 17:09:08,797 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 157 to namenode at http://hdp-01:50070 in 0.006 seconds
2019-10-24 17:09:08,797 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-24 18:09:09,088 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 18:09:09,088 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=158&endTxId=159&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 18:09:09,099 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-24 18:09:09,099 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000158-0000000000000000159_0000000000589494364 size 0 bytes.
2019-10-24 18:09:09,099 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 18:09:09,100 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000158-0000000000000000159 expecting start txid #158
2019-10-24 18:09:09,100 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000158-0000000000000000159
2019-10-24 18:09:09,100 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000158-0000000000000000159 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 18:09:09,106 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 157
2019-10-24 18:09:09,106 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000155, cpktTxId=0000000000000000155)
2019-10-24 18:09:09,118 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 159 to namenode at http://hdp-01:50070 in 0.009 seconds
2019-10-24 18:09:09,118 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-24 19:09:09,390 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 19:09:09,390 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=160&endTxId=161&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 19:09:09,394 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-24 19:09:09,394 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000160-0000000000000000161_0000000000593094666 size 0 bytes.
2019-10-24 19:09:09,395 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 19:09:09,395 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000160-0000000000000000161 expecting start txid #160
2019-10-24 19:09:09,395 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000160-0000000000000000161
2019-10-24 19:09:09,395 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000160-0000000000000000161 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 19:09:09,402 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 159
2019-10-24 19:09:09,402 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000157, cpktTxId=0000000000000000157)
2019-10-24 19:09:09,413 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 161 to namenode at http://hdp-01:50070 in 0.009 seconds
2019-10-24 19:09:09,413 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-24 20:09:09,799 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 20:09:09,800 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=162&endTxId=163&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 20:09:09,804 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-24 20:09:09,804 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000162-0000000000000000163_0000000000596695075 size 0 bytes.
2019-10-24 20:09:09,804 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 20:09:09,804 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000162-0000000000000000163 expecting start txid #162
2019-10-24 20:09:09,804 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000162-0000000000000000163
2019-10-24 20:09:09,804 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000162-0000000000000000163 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 20:09:09,852 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 161
2019-10-24 20:09:09,853 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000159, cpktTxId=0000000000000000159)
2019-10-24 20:09:09,859 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 163 to namenode at http://hdp-01:50070 in 0.005 seconds
2019-10-24 20:09:09,860 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-24 21:09:10,137 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 21:09:10,137 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=164&endTxId=165&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 21:09:10,142 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-24 21:09:10,142 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000164-0000000000000000165_0000000000600295413 size 0 bytes.
2019-10-24 21:09:10,142 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 21:09:10,142 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000164-0000000000000000165 expecting start txid #164
2019-10-24 21:09:10,142 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000164-0000000000000000165
2019-10-24 21:09:10,142 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000164-0000000000000000165 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 21:09:10,148 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 163
2019-10-24 21:09:10,148 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000161, cpktTxId=0000000000000000161)
2019-10-24 21:09:10,161 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 165 to namenode at http://hdp-01:50070 in 0.011 seconds
2019-10-24 21:09:10,161 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-24 22:09:10,369 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 22:09:10,369 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=166&endTxId=167&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 22:09:10,381 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-24 22:09:10,381 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000166-0000000000000000167_0000000000603895645 size 0 bytes.
2019-10-24 22:09:10,381 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 22:09:10,381 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000166-0000000000000000167 expecting start txid #166
2019-10-24 22:09:10,381 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000166-0000000000000000167
2019-10-24 22:09:10,381 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000166-0000000000000000167 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 22:09:10,393 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 165
2019-10-24 22:09:10,393 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000163, cpktTxId=0000000000000000163)
2019-10-24 22:09:10,434 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 167 to namenode at http://hdp-01:50070 in 0.029 seconds
2019-10-24 22:09:10,434 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-24 23:09:10,735 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-24 23:09:10,735 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=168&endTxId=169&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-24 23:09:10,762 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2019-10-24 23:09:10,762 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000168-0000000000000000169_0000000000607496010 size 0 bytes.
2019-10-24 23:09:10,762 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-24 23:09:10,763 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000168-0000000000000000169 expecting start txid #168
2019-10-24 23:09:10,763 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000168-0000000000000000169
2019-10-24 23:09:10,763 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000168-0000000000000000169 of size 42 edits # 2 loaded in 0 seconds
2019-10-24 23:09:10,768 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 167
2019-10-24 23:09:10,768 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000165, cpktTxId=0000000000000000165)
2019-10-24 23:09:10,794 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 169 to namenode at http://hdp-01:50070 in 0.023 seconds
2019-10-24 23:09:10,794 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1416
2019-10-25 00:09:11,534 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2019-10-25 00:09:11,534 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://hdp-01:50070/imagetransfer?getedit=1&startTxId=170&endTxId=177&storageInfo=-63:17681246:0:CID-545e7da7-70e2-4505-8d5d-c69281feadf4
2019-10-25 00:09:11,596 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.03s at 0.00 KB/s
2019-10-25 00:09:11,596 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000170-0000000000000000177_0000000000611096810 size 0 bytes.
2019-10-25 00:09:11,596 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2019-10-25 00:09:11,597 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000170-0000000000000000177 expecting start txid #170
2019-10-25 00:09:11,597 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000170-0000000000000000177
2019-10-25 00:09:11,598 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-root/dfs/namesecondary/current/edits_0000000000000000170-0000000000000000177 of size 602 edits # 8 loaded in 0 seconds
2019-10-25 00:09:11,691 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 169
2019-10-25 00:09:11,691 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-root/dfs/namesecondary/current/fsimage_0000000000000000167, cpktTxId=0000000000000000167)
2019-10-25 00:09:11,709 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 177 to namenode at http://hdp-01:50070 in 0.01 seconds
2019-10-25 00:09:11,709 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 1744
